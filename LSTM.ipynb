{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uNK_K9QW3P1u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Activation, BatchNormalization, GRU, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset for electrode 1 (E1)\n",
        "dataset_e1= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E1.csv\",delimiter=\",\")\n",
        "dataset_e1=np.array(dataset_e1,float)\n",
        "X_e1 = dataset_e1[:,1:6]\n",
        "# Load dataset for electrode 2 (E2)\n",
        "dataset_e2= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E2.csv\",delimiter=\",\")\n",
        "dataset_e2=np.array(dataset_e2,float)\n",
        "X_e2 = dataset_e2[:,1:6]\n",
        "# Extract output (Y)\n",
        "Y= dataset_e1[:,8]\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "s2vdlrpm32-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "# Step1\n",
        "lin,col =np.shape(X_e1)\n",
        "for i in range(0, col-2):\n",
        "    X_e1[:, i] = (X_e1[:, i]/dataset_e1[:, 0])\n",
        "    X_e2[:, i] = (X_e2[:, i]/dataset_e2[:, 0])\n",
        "\n",
        "# Step 2: Use StandardScaler for further normalization\n",
        "scaler1 = StandardScaler().fit(X_e1)\n",
        "scaler2 = StandardScaler().fit(X_e2)\n",
        "X_e1 = scaler1.transform(X_e1)\n",
        "X_e2 = scaler1.transform(X_e2)\n",
        "\n",
        "# Step 3: Combine normalized data into a 3D array\n",
        "X=np.zeros((X_e1.shape[0],X_e1.shape[1],2))\n",
        "X[:,:,0]=X_e1\n",
        "X[:,:,1]=X_e2\n",
        "\n",
        "print(\"Shape of X_e1:\", X_e1.shape)\n",
        "print(\"Shape of X_e2:\", X_e2.shape)\n",
        "print(\"Shape of X:\",X.shape)"
      ],
      "metadata": {
        "id": "bgOZlafQ5CDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "x_train_new=x_train.copy()\n",
        "x_test_new = x_test.copy()\n",
        "# Convert labels to categorical\n",
        "Y_train_new = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test_new = tf.keras.utils.to_categorical(y_test)\n",
        "print(\"Shape of y_train:\",y_train.shape)\n",
        "print(\"Shape of Y_train_new:\", Y_train_new.shape)"
      ],
      "metadata": {
        "id": "EjBkiQ2m5KG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the training and testing data for CNN input\n",
        "x_train_new=x_train_new.reshape(x_train_new.shape[0],  x_train_new.shape[1], 2)\n",
        "x_test_new = x_test_new.reshape(x_test_new.shape[0], x_test_new.shape[1],2 )\n",
        "print (x_train_new.shape)"
      ],
      "metadata": {
        "id": "EVktjwJm6UDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add LSTM layers\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(5, 2), activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(32, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(32, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(16))\n",
        "\n",
        "# Add Dense and Activation layers\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,\n",
        "                                        beta_1=0.9,\n",
        "                                        beta_2=0.999,\n",
        "                                        epsilon=1e-07,\n",
        "                                        amsgrad=False,\n",
        "                                        name='Adam'),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rtUEk55Nrxle"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path for saving the best weights\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Code_IA_Test/best_weights.h5'\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "checkpointer = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                               verbose=1,\n",
        "                               monitor='val_loss',\n",
        "                               mode='auto',\n",
        "                               save_best_only=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(x_train_new, Y_train_new,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.20,\n",
        "                    callbacks=[checkpointer])\n",
        "\n",
        "# save the entire model\n",
        "# model.save('weights_LSTM.h5')"
      ],
      "metadata": {
        "id": "Hsf9Rzfs68Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "\n",
        "# Display the test loss and accuracy\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "2HD3p1tJ9nuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mw23210K9yQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend( loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dU7AlLGb-o5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test_new, batch_size=128, verbose=2)\n",
        "print (\"Predicted data shape:\", predictions.shape)\n",
        "print(\"True labels shape:\", Y_test_new.shape)"
      ],
      "metadata": {
        "id": "B7l8atdb-06l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the custom threshold for positive predictions\n",
        "threshold_confusion = 0.5\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "\n",
        "# Initialize arrays to store predicted and true labels\n",
        "y_pred = np.empty((predictions.shape[0]))\n",
        "y_test = np.empty((predictions.shape[0]))\n",
        "\n",
        "# Populate the arrays with predicted and true labels\n",
        "for i in range(predictions.shape[0]):\n",
        "    y_pred[i]=np.argmax(predictions[i])\n",
        "    y_test[i]=np.argmax(Y_test_new[i])\n",
        "\n",
        "# Calculate and display the confusion matrix\n",
        "confusion = confusion_matrix(y_test,  y_pred)\n",
        "print (confusion)"
      ],
      "metadata": {
        "id": "adVnf5AkAQ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"The accuracy score on this random test-set is:\", accuracy)\n",
        "\n",
        "# Calculate and display recall\n",
        "recall = recall_score(y_test, y_pred, average=None)\n",
        "average_recall = sum(recall) / len(recall)\n",
        "print(\"Recall:\", recall, \"The average recall is\", average_recall)\n",
        "\n",
        "# Calculate and display precision\n",
        "precision = precision_score(y_test, y_pred, average=None)\n",
        "average_precision = sum(precision) / len(precision)\n",
        "print(\"Precision:\", precision, \"The average precision is\", average_precision)\n",
        "\n",
        "# Calculate and display F1 score\n",
        "f1_score = (2 * average_precision * average_recall) / (average_precision + average_recall)\n",
        "print(\"F1 Score:\", f1_score)"
      ],
      "metadata": {
        "id": "PzSoYY48AphO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}