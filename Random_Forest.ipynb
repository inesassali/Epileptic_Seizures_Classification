{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Conv1D,Conv2D, MaxPooling1D,Flatten,BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "numpy.random.seed(2)"
      ],
      "metadata": {
        "id": "qdOtHNgUCshc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset for electrode 1 (E1)\n",
        "dataset_e1= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E1.csv\",delimiter=\",\")\n",
        "dataset_e1=numpy.array(dataset_e1,float)\n",
        "X_e1 = dataset_e1[:,1:7]\n",
        "# Load dataset for electrode 2 (E2)\n",
        "dataset_e2= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E2.csv\",delimiter=\",\")\n",
        "dataset_e2=numpy.array(dataset_e2,float)\n",
        "X_e2 = dataset_e2[:,1:7]\n",
        "# Extract output (Y)\n",
        "Y= dataset_e1[:,8]\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "QqdEZ3RZaZLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels Histogram\n",
        "Y_histogram = Counter(Y)\n",
        "print(\"Labels Histogram:\", Y_histogram)\n",
        "plt.bar(Y_histogram.keys(), Y_histogram.values(), width=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TkEWiXMEIVnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportions of each class\n",
        "interictal_percentage = np.sum(Y == 0) / len(Y)\n",
        "preictal_percentage = np.sum(Y == 1) / len(Y)\n",
        "ictal_percentage = np.sum(Y == 2) / len(Y)\n",
        "\n",
        "# Print the average values of the Y column\n",
        "print('Interictal: {0:.3f} | Preictal: {1:.3f} | Ictal: {2:.3f}'.format(interictal_percentage, preictal_percentage, ictal_percentage))"
      ],
      "metadata": {
        "id": "Q7JAoaN2KZkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Normalize data by dividing each column by the first column of the respective dataset\n",
        "def normalize_data(X, dataset):\n",
        "    lin, col = np.shape(X)\n",
        "    for i in range(col - 2):\n",
        "        X[:, i] = (X[:, i] / dataset[:, 0])\n",
        "\n",
        "# Assuming dataset_e1 and dataset_e2 are defined\n",
        "\n",
        "# Normalize data for X_e1 and X_e2\n",
        "normalize_data(X_e1, dataset_e1)\n",
        "normalize_data(X_e2, dataset_e2)\n",
        "\n",
        "# Step 2: Use StandardScaler for further normalization\n",
        "scaler1 = StandardScaler().fit(X_e1)\n",
        "scaler2 = StandardScaler().fit(X_e2)\n",
        "X_e1 = scaler1.transform(X_e1)\n",
        "X_e2 = scaler2.transform(X_e2)\n",
        "\n",
        "# Step 3: Combine normalized data into a 3D array\n",
        "X = np.stack((X_e1, X_e2), axis=-1)\n",
        "\n",
        "print(\"Shape of X_e1:\", X_e1.shape)\n",
        "print(\"Shape of X_e2:\", X_e2.shape)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "\n",
        "# Step 4: Reshape the 3D array into a 2D array\n",
        "nsamples, nfeatures, nchannels = X.shape\n",
        "X_norm = X.reshape((nsamples, nfeatures * nchannels))\n",
        "\n",
        "print(\"Shape of X_norm:\", X_norm.shape)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RhSBmF9XzU6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into 80% training and 20% (testing and validating)\n",
        "# Step 1: Split into 80% training and 20% (testing + validating)\n",
        "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X_norm, Y, test_size=0.2)\n",
        "\n",
        "# Step 2: Further split the 20% into 10% validation and 10% testing\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)\n",
        "\n",
        "# Print shapes of the resulting datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of Y_val:\", Y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of Y_test:\", Y_test.shape)\n"
      ],
      "metadata": {
        "id": "Q3kdZ31uzcfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestClassifier with specified parameters\n",
        "# n_estimators: The number of trees in the forest\n",
        "# criterion: The function to measure the quality of a split (here, 'entropy' for information gain)\n",
        "# random_state: Seed for random number generation, providing reproducibility\n",
        "model = RandomForestClassifier(n_estimators=1000, criterion='entropy', random_state=0)"
      ],
      "metadata": {
        "id": "QZd4nwOjzmAJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the RandomForestClassifier model using the training data\n",
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "_MAhOLG7zoAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the trained model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate various metrics\n",
        "accuracy = accuracy_score(predictions, Y_test)\n",
        "precision = precision_score(predictions, Y_test, average='macro')\n",
        "recall = recall_score(predictions, Y_test, average='macro')\n",
        "f1 = f1_score(predictions, Y_test, average='macro')\n",
        "\n",
        "# Print metric scores\n",
        "print(\"Metric scores:\")\n",
        "print(\" Accuracy: {:.2f} %\".format(accuracy * 100))\n",
        "print(\" Precision: {:.2f} %\".format(precision * 100))\n",
        "print(\" Recall: {:.2f} %\".format(recall * 100))\n",
        "print(\" F1: {:.2f} %\".format(f1 * 100))\n",
        "\n",
        "# Generate and print a detailed classification report\n",
        "clf_report = classification_report(predictions, Y_test)\n",
        "print(clf_report)"
      ],
      "metadata": {
        "id": "rumjnjZnMq-o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}