{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2lq1NI-xMv-x"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Conv1D,Conv2D, MaxPooling1D,Flatten,BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "numpy.random.seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset for electrode 1 (E1)\n",
        "dataset_e1= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E1.csv\",delimiter=\",\")\n",
        "dataset_e1=numpy.array(dataset_e1,float)\n",
        "X_e1 = dataset_e1[:,1:6]\n",
        "# Load dataset for electrode 2 (E2)\n",
        "dataset_e2= pd.read_csv(\"/content/drive/MyDrive/Code_IA_Test/E2.csv\",delimiter=\",\")\n",
        "dataset_e2=numpy.array(dataset_e2,float)\n",
        "X_e2 = dataset_e2[:,1:6]\n",
        "# Extract output (Y)\n",
        "Y= dataset_e1[:,8]\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "rY-iCuMlNxiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "# Step1\n",
        "lin,col =np.shape(X_e1)\n",
        "for i in range(0, col-2):\n",
        "    X_e1[:, i] = (X_e1[:, i]/dataset_e1[:, 0])\n",
        "    X_e2[:, i] = (X_e2[:, i]/dataset_e2[:, 0])\n",
        "\n",
        "# Step 2: Use StandardScaler for further normalization\n",
        "scaler1 = StandardScaler().fit(X_e1)\n",
        "scaler2 = StandardScaler().fit(X_e2)\n",
        "X_e1 = scaler1.transform(X_e1)\n",
        "X_e2 = scaler1.transform(X_e2)\n",
        "\n",
        "# Step 3: Combine normalized data into a 3D array\n",
        "X=np.zeros((X_e1.shape[0],X_e1.shape[1],2))\n",
        "X[:,:,0]=X_e1\n",
        "X[:,:,1]=X_e2\n",
        "\n",
        "print(\"Shape of X_e1:\", X_e1.shape)\n",
        "print(\"Shape of X_e2:\", X_e2.shape)\n",
        "print(\"Shape of X:\",X.shape)"
      ],
      "metadata": {
        "id": "BakYtXsoN1Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "x_train_new=x_train.copy()\n",
        "x_test_new = x_test.copy()\n",
        "# Convert labels to categorical\n",
        "Y_train_new = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test_new = tf.keras.utils.to_categorical(y_test)\n",
        "print(\"Shape of y_train:\",y_train.shape)\n",
        "print(\"Shape of Y_train_new:\", Y_train_new.shape)"
      ],
      "metadata": {
        "id": "uOSwZ_TCS5pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layer 1\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu',data_format=\"channels_last\",padding='same',input_shape=(5,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional layer 2\n",
        "model.add(Conv1D(64, 3, activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())  # Batch normalization for improved training stability\n",
        "model.add(MaxPooling1D(pool_size=2,strides=None))  # Max pooling to reduce spatial dimensions\n",
        "model.add(Dropout(0.5)) # Dropout for regularization to prevent overfitting\n",
        "\n",
        "# Convolutional layer 3\n",
        "model.add(Conv1D(128, 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Flatten layer to transition from convolutional layers to fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer with softmax activation for multiclass classification\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1Jo4BkfaU0Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cross-validation using 3 folds\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "for train_index, val_index in kf.split(x_train_new):\n",
        "    # Split the data into training and validation sets\n",
        "    x_train, x_val = x_train_new[train_index], x_train_new[val_index]\n",
        "    Y_train, y_val = Y_train_new[train_index], Y_train_new[val_index]\n",
        "\n",
        "    # Further split the training set into training and testing sets\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, Y_train, test_size=0.15, random_state=42)\n",
        "\n",
        "    # Reshape the data to fit the model\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 2))\n",
        "    x_val= np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 2))\n",
        "\n",
        "    # Train the model on the current training data\n",
        "    checkpointer = ModelCheckpoint(filepath='./'+'_best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
        "\n",
        "    # Save the best weights based on validation loss\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32, callbacks=[checkpointer])"
      ],
      "metadata": {
        "id": "j9P6SzZpYhr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "\n",
        "# Display the test loss and accuracy\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "rVL2hCysV8bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJJQQTXtWClx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend( loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Woba4EPKWJ2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test_new, batch_size=64, verbose=2)\n",
        "print (\"Predicted data shape:\", predictions.shape)\n",
        "print(\"True labels shape:\", Y_test_new.shape)"
      ],
      "metadata": {
        "id": "wNWKbqmhXE1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the custom threshold for positive predictions\n",
        "threshold_confusion = 0.5\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "\n",
        "# Initialize arrays to store predicted and true labels\n",
        "y_pred = np.empty((predictions.shape[0]))\n",
        "y_test = np.empty((predictions.shape[0]))\n",
        "\n",
        "# Populate the arrays with predicted and true labels\n",
        "for i in range(predictions.shape[0]):\n",
        "    y_pred[i]=np.argmax(predictions[i])\n",
        "    y_test[i]=np.argmax(Y_test_new[i])\n",
        "\n",
        "# Calculate and display the confusion matrix\n",
        "confusion = confusion_matrix(y_test,  y_pred)\n",
        "print (confusion)"
      ],
      "metadata": {
        "id": "qfh4PSu1XH1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"The accuracy score on this random test-set is:\", accuracy)\n",
        "\n",
        "# Calculate and display recall\n",
        "recall = recall_score(y_test, y_pred, average=None)\n",
        "average_recall = sum(recall) / len(recall)\n",
        "print(\"Recall:\", recall, \"The average recall is\", average_recall)\n",
        "\n",
        "# Calculate and display precision\n",
        "precision = precision_score(y_test, y_pred, average=None)\n",
        "average_precision = sum(precision) / len(precision)\n",
        "print(\"Precision:\", precision, \"The average precision is\", average_precision)\n",
        "\n",
        "# Calculate and display F1 score\n",
        "f1_score = (2 * average_precision * average_recall) / (average_precision + average_recall)\n",
        "print(\"F1 Score:\", f1_score)"
      ],
      "metadata": {
        "id": "qYdFHFhGXO8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}